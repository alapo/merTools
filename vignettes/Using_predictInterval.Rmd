---
title: "Prediction Intervals from merMod Objects"
author: "Jared Knowles and Carl Frederick"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Intro}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(
  cache=TRUE,
  comment="#>",
  collapse=TRUE
)
library(abind); library(mvtnorm); library(knitr); library(merTools)
```

Fitting (generalized) linear mixed models, (G)LMM, to very large data sets is becomming increasingly easy, but understanding and
communicating the uncertainty inherent in those models is not. As the documentation for `lme4::predict.merMod()` notes:

>There is no option for computing standard errors of predictions because it is difficult to define an efficient method that >incorporates uncertainty in the variance parameters; we recommend `lme4::bootMer()` for this task.

We agree that, short of a fully bayesian analysis, bootstrapping is the gold-standard for deriving a prediction interval 
predictions from a (G)LMM, but the time required to obtain even a respectable number of replications quickly becomes prohibitive when the initial model fit is on the order of hours instead of seconds. The only(?) other alternative out there is to use the `arm::sim()` function to simulate values, but this only takes variation of the fixed coefficients and residuals into account. 

We developed this function to incorporate the variation in the conditional modes of the random effects (CMRE, a.k.a. BLUPs in the LMM case) into calculating prediction intervals. We take the warning from `lme4::predict.merMod()` seriously, but view this method as a decent first approximation the full bootstrap analysis for (G)LMMs fit to very large data sets.

*...we could add some stuff here about consequential decisions being made about the fitted random effects estimates and the lack of availability of quick methods...*

## Conceptual description

There are three sources of uncertainty in mixed models:
  (1) the residual (observation-level) variance,
  (2) the uncertainty in the fixed coefficients, and
  (3) the uncertainty in the variance parameters for the grouping factors.
As we mentioned above, the `arm:sim()` function incorporates the first two sources of variation but not the third and bootstrapping methods using `lme4::bootMer()` does incorporate all three sources of uncertainty because it re-estimates the model using random samples of the data.

When inference about the values of the CMREs is of interest, it would be nice to incorporate some degree of uncertainty in 
those estimates when comparing observations across groups. `predictInterval()` does this by drawing values of the CMREs from 
the conditional variance-covariance matrix of the random affects accessible from `arm::ranef(model, condVar=TRUE)`. Thus, `predictInterval()` incorporates all of the uncertainty from sources one and two, and part of the variance from source 3, but the variance parameters themselves are treated as fixed.

Essentially, `predictInterval()` takes an estimated model of class "merMod" and, like `predict()`, a data.frame upon which
to make those predictions and:
  - *First*  extracts the fixed and random coefficients
  - *Second* takes `nsims` draws from the multivariate normal distribution of the fixed and random coefficients (separately)
  - *Third*  calculates the linear predictor for each row in `newdata` based on these draws and
  - *Fourth* either incorporates the residual variation (per the `arm::sim()` function) or, if specified by the user, not
  - *Fifth*  returns newdata with the lower and upper limits of the prediction interval and the mean or median of the simulated              predictions

Currently, the supported model types are linear mixed models and mixed logistic regression models.  

The prediction data set *can* include levels that are not in the estimation model frame. The prediction intervals for such observations only incorporate uncertainty from fixed coefficient estimates and the residual level variation.

## Comparison to existing methods

What do the differences between `predictInterval()` and the other methods for constructing prediction intervals mean in practice?  This section compares the results of `predictInterval()` with those obtained using `arm::sim()` and `lme4::bootMer()` using the sleepstudy data from `lme4`.  These data contain reaction time observations for 10 days on 18 subjects.  The data are sorted such that the first 10 observations are days one through ten for subject 1, the next 10 are days one through ten for subject 2 and so on. The toy model that we are estimating below estimates random intercepts and a random slope for the number of days.

###Step 1: Estimating the model and using `predictInterval()`

First, we will load the required packages and data and estimate the model:

```{r Prep, message=FALSE}
library(lme4); library(arm); 
##library(merTools);
set.seed(271828)
data(sleepstudy)
fm1 <- lmer(Reaction ~ Days + (Days|Subject), data=sleepstudy)
display(fm1)
```

Then, calculate prediction intervals using `predictInterval()`.  For this example we will calculate the prediction interval for the original data.frame.

```{r predInt}
PI.time <- system.time(
  PI <- predictInterval(model = fm1, newdata = sleepstudy, 
                        level = 0.95, nsim = 1000,
                        stat = "median", predict.type="linear.prediction",
                        include.resid.var = TRUE)
)
```

Here is the first few rows of the object `PI`:
```{r Inspect predInt, results="asis", echo=FALSE}
kable(head(PI))
```

The final three columns are the median (`fit`) and limits of the 95% prediction interval (`upr` and `lwr`). The following figure displays the output graphically for the first 30 observations.

```{r Inspect predInt 2, fig.width=7, fig.align="center"}
library(ggplot2);
ggplot(aes(x=1:30, y=fit, ymin=lwr, ymax=upr), data=PI[1:30,]) +
  geom_point() + 
  geom_linerange() +
  labs(x="Index", y="Prediction w/ 95% PI")
```

###Step 2: Comparison with `arm::sim()`

How does the output above compare to what we could get from `arm::sim()`?

```{r arm.Sim, fig.width=7, fig.height=4, fig.align="center"}
PI.arm.time <- system.time(
  PI.arm.sims <- arm::sim(fm1, 1000)
)

PI.arm <- data.frame(
  fit=apply(fitted(PI.arm.sims, fm1), 1, function(x) quantile(x, 0.500)),
  upr=apply(fitted(PI.arm.sims, fm1), 1, function(x) quantile(x, 0.975)),
  lwr=apply(fitted(PI.arm.sims, fm1), 1, function(x) quantile(x, 0.025))
)

comp.data <- rbind(data.frame(Predict.Method="predictInterval()", x=(1:nrow(PI))-0.1, PI[,4:6]),
                   data.frame(Predict.Method="arm::sim()", x=(1:nrow(PI.arm))+0.1, PI.arm))

ggplot(aes(x=x, y=fit, ymin=lwr, ymax=upr, color=Predict.Method), data=comp.data[c(1:30,181:210),]) +
  geom_point() + 
  geom_linerange() +
  labs(x="Index", y="Prediction w/ 95% PI") +
  theme(legend.position="bottom")
          
              
```

The prediction intervals from `arm:sim()` are much smaller and the random slope for days vary more than they do for predictInterval. Both results are as expected, given the small number of subjects and observations per subject in these data. Because `predictInterval()` is incorporating uncertainty in the CMFEs (but not the variance parameters of the random coefficients themselves), the Days slopes are closer to the overall or pooled regression slope.  

###Step 3: Comparison with `lme4::bootMer()`

As quoted above, the developers of lme4 suggest that users interested in uncertainty estimates around their predictions use `lme4::bootmer()` to calculate them. The documentation for `lme4::bootMer()` goes on to describe three implemented flavors of bootstrapped estimates:

  (1) parametrically resampling both the *"spherical"* random effects *u* and the i.i.d. errors $\epsilon$
  (2) treating the random effects as fixed and parametrically resampling the i.i.d. errors 
  (3) treating the random effects as fixed and semi-parametrically resampline the i.i.d. errors from the distribution of residuals.
  
We will compare the results from `predictInterval()` with each method, in turn.

####Step 3a: `lme4::bootMer()` method 1

```{r bootMer.1, fig.width=7, fig.height=4, fig.align="center"}
##Functions for bootMer() and objects
####Return predicted values from bootstrap
mySumm <- function(.) {
  predict(., newdata=sleepstudy, re.form=NULL)
}
####Collapse bootstrap into median, 95% PI
sumBoot <- function(merBoot) {
  return(
    data.frame(fit = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.5, na.rm=TRUE))),
               lwr = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.025, na.rm=TRUE))),
               upr = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.975, na.rm=TRUE)))
    )
  )
}

##lme4::bootMer() method 1
PI.boot1.time <- system.time(
  boot1 <- lme4::bootMer(fm1, mySumm, nsim=1000, use.u=FALSE, type="parametric")
)

PI.boot1 <- sumBoot(boot1)

comp.data <- rbind(data.frame(Predict.Method="predictInterval()", x=(1:nrow(PI))-0.1, PI[,4:6]),
                   data.frame(Predict.Method="lme4::bootMer() - Method 1", x=(1:nrow(PI.boot1))+0.1, PI.boot1))

ggplot(aes(x=x, y=fit, ymin=lwr, ymax=upr, color=Predict.Method), data=comp.data[c(1:30,181:210),]) +
  geom_point() + 
  geom_linerange() +
  labs(x="Index", y="Prediction w/ 95% PI") +
  theme(legend.position="bottom")
  
```

*some notes re: diffs here*

####Step 3b: `lme4::bootMer()` method 2

```{r bootMer.2, fig.width=7, fig.height=4, fig.align="center"}
##lme4::bootMer() method 2
PI.boot2.time <- system.time(
  boot2 <- lme4::bootMer(fm1, mySumm, nsim=1000, use.u=TRUE, type="parametric")
)

PI.boot2 <- sumBoot(boot2)

comp.data <- rbind(data.frame(Predict.Method="predictInterval()", x=(1:nrow(PI))-0.1, PI[,4:6]),
                   data.frame(Predict.Method="lme4::bootMer() - Method 2", x=(1:nrow(PI.boot2))+0.1, PI.boot2))

ggplot(aes(x=x, y=fit, ymin=lwr, ymax=upr, color=Predict.Method), data=comp.data[c(1:30,181:210),]) +
  geom_point() + 
  geom_linerange() +
  labs(x="Index", y="Prediction w/ 95% PI") +
  theme(legend.position="bottom")
  
```

*some notes re: diffs here*

####Step 3c: `lme4::bootMer()` method 3

```{r bootMer.3, fig.width=7, fig.height=4, fig.align="center"}
##lme4::bootMer() method 3
PI.boot3.time <- system.time(
  boot3 <- lme4::bootMer(fm1, mySumm, nsim=1000, use.u=TRUE, type="semiparametric")
)

PI.boot3 <- sumBoot(boot3)

comp.data <- rbind(data.frame(Predict.Method="predictInterval()", x=(1:nrow(PI))-0.1, PI[,4:6]),
                   data.frame(Predict.Method="lme4::bootMer() - Method 3", x=(1:nrow(PI.boot3))+0.1, PI.boot3))

ggplot(aes(x=x, y=fit, ymin=lwr, ymax=upr, color=Predict.Method), data=comp.data[c(1:30,181:210),]) +
  geom_point() + 
  geom_linerange() +
  labs(x="Index", y="Prediction w/ 95% PI") +
  theme(legend.position="bottom")
  
```

*some notes re: diffs here*

###Computation time

Our initial motivation for writing this function was to develop a method for incorporating uncertainty in the CMFEs  for mixed models estimated on very large samples. Even for models with only modest degrees of complexity, using `lme4::bootMer()` quickly becomes time prohibitive because it involves re-estimating the model for each simulation. We have seen how each alternative compares to `predictInterval()` substantively, but how do they compare in terms of computational time? The table below lists the output of `system.time()` for all five methods for calculating prediction intervals for `merMod` objects.

```{r, echo=FALSE}
times <- rbind(PI.time, PI.arm.time, PI.boot1.time, PI.boot2.time, PI.boot3.time)[,1:3]
rownames(times) <- c("predictInterval()", "arm::sim()", "lme4::bootMer()-Method 1", "lme4::bootMer()-Method 2", "lme4::bootMer()-Method 3")
kable(times)
```

For this simple example, we see that `arm::sim()` is the fastest--nearly five times faster than `predictInterval()`. However, `predictInterval()` is nearly six times faster than any of the bootstrapping options via `lme4::bootMer`.  This may not seem like a lot, but consider that the computational time for required for bootstrapping is roughly proportional to the number of bootstrapped simulations requested ... `predictInterval()` is not because it is just a series of draws from various multivariate normal distributions, so the time ratios in the table below represents the lowest bound of the computation time ratio of bootstrapping to `predictInterval()`.
